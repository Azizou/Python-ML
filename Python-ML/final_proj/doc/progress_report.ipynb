{
 "metadata": {
  "name": "final_project"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown", 
     "source": [
      "#Progress report for Machine Learning final project of students Sarthak Grover and Jason Filippou", 
      "", 
      "##Theme: Estimating unsupervised learning and feature representation accuracy on an image dataset.", 
      "", 
      "####I) Abstract: ", 
      "$$", 
      "$$", 
      "In this project, we attempt to estimate the degree to which certain unsupervised learning algorithms learn the \"correct classes\" for the data on which", 
      "they are applied. By \"correct classes\", we effectively mean \"clusters which accurately reflect the real labels\". Our procedure can be summarized at a high-level as ", 
      "follows: given a labeled dataset, we strip the feature vectors from their labels and run a battery of unsupervised learning algorithms in order to predict", 
      "clusters of our data. We then apply a set of metrics to estimate how well the predicted clusters reflect the true labels of our dataset. The dataset that", 
      "we use to perform our tests is the popular image dataset Caltech101: http://www.vision.caltech.edu/Image_Datasets/Caltech101/", 
      "", 
      "When trying to cluster images into categories, feature extraction is very important. A prevalent problem in Computer Vision is that areas in images", 
      "are highly inter-dependent, therefore feature extraction has to be such that these dependencies are taken into consideration. In order to investigate how our", 
      "unsupervised learning techniques can benefit from different feature extraction methods, we apply the same algorithms on two versions of Caltech101: a) A gradient-based feature extraction", 
      "provided to us from the course TA Mohammad Rastegari as part of project 2 and b) Feature extraction based on Scale-Invariant Feature Transform (SIFT, ", 
      "http://en.wikipedia.org/wiki/Scale-invariant_feature_transform), downloaded from http://files.is.tue.mpg.de/pgehler/projects/iccv09/. SIFT has gained", 
      "significant popularity in the Computer Vision domain, and has been packaged in the award-winning open source library VLFeat (http://www.vlfeat.org/); we therefore", 
      "believe that this work is also useful in that it examines whether SIFT features accurately reflect the peculiarities of the input image."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "####II) Methodology employed:", 
      "$$", 
      "$$", 
      "We believe that the following unsupervised learning algorithms are suitable for experimentation given our timeframe:", 
      "", 
      "", 
      "  1) K-means, where we hardcode $k=101\\ $ with aim to produce as many clusters as there are labels in our dataset.", 
      "", 
      "  2) Gaussian Mixture Model", 
      "", 
      "  3) A modification of the Gaussian Mixture Model fit to work with inter-cluster dependencies (i.e $k$ covariance matrices instead of simple scalar variances)."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "####III) Data and code:", 
      "$$", 
      "$$", 
      "We have the data downloaded and pre-processed to fit our needs. Our data consists of 101 image categories of Caltech, both in gradients format (588 - dimensional", 
      "feature vectors) as well as in SIFT format, with two different bin discretizations of 1000 and 300. We have currently experimented only on the 1000-dimensional version", 
      "of the SIFT data and intend to experiment on the 300-dimensional version as well.", 
      "", 
      "We have also written code that evaluates any given clustering technique by a pair of metrics. We explain those metrics in detail in section (IV)", 
      "of this notebook, where we also provide informative snippets of code as well as code cells that allow our code to be run. The entirety of our code base ", 
      "has been written in Python and makes extensive use of SciPy for numerical computations and clustering."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "####IV) Progress so far:", 
      "", 
      "We have been able to make a thorough K-means based experimentation on both versions of the Caltech data. Specifically, we have run K-means with 100 random ", 
      "restarts, K = 101 and a stopping criterion of the loss function changing less than $10^{-5}$ on both versions of the Caltech dataset. We are using", 
      "SciPy's k-means implementation, contained in the module scipy.cluster.vq (http://docs.scipy.org/doc/scipy/reference/cluster.vq.html). We evaluate the output", 
      "of K-means with respect to the examples' true labels by the following two metrics:", 
      "", 
      "1. _Wrong cluster prediction as misclassification error_: Since we have the true classes of our dataset, we are able to calculate, for every such class, the", 
      "mean point in vector space. Once that is done, it is possible to calculate the distance between every centroid that K-means predicted and that mean. We then", 
      "associate every cluster with the label whose mean it is closest to, and estimate the clustering \"accuracy\" in terms of misclassification error. One positive", 
      "element of this approach is that it allows the integration of well established metrics such as Precision and Recall in the evaluation process. One negative", 
      "element is that it penalizes the clustering algorithm heavily: \"wrong\" large clusters with approximately the same distribution of true labels amongst their points", 
      "(or, even worse, unbalanced distributions with the majority points being of a class different than the one that the cluster predicts) will contribute ", 
      "greatly to the misclassifications.", 
      "", 
      "2. _Estimating accurate clusters as being close to their majority true label_: As mentioned previously, it's possible to compute the closest label mean", 
      "to any given cluster. After doing that, we build a histogram of the distribution of the true labels inside the cluster. If the maximal true label reported by", 
      "the points assigned to this cluster is the same label whose mean is the closest to the cluster, we mark that cluster as an \"accurate\" cluster. This metric", 
      "has the benefit that it covers the intuitive meaning of an \"accurate\" cluster in that, for such a cluster, there exists a correlation between the majority", 
      "label of its points and the label it is closest to in vector space. In addition, it is easily converted to a \"weighted\" version, by computing an \"accuracy percentage\" of each", 
      "cluster based on the fraction of its maximal label points given the entirety of points. One drawback is that well-established metrics like the F-measure ", 
      "cannot be easily applied to it. ", 
      "", 
      "In order to get the best of both worlds, we use both metrics in our code. The following MarkDown cell contains our current implementation for the ", 
      "evaluation of the clustering algorithm:"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "<pre><code>", 
      "from __future__ import division ", 
      "'''", 
      "Created on Dec 3, 2012", 
      "", 
      "@author: jason", 
      "'''", 
      "from util.mlExceptions import *", 
      "import os", 
      "from inspect import stack", 
      "from scipy.io import loadmat", 
      "import pickle as pkl", 
      "from scipy.cluster.vq import *", 
      "from collections import Counter", 
      "from numpy.linalg import norm", 
      "import numpy as np", 
      "", 
      "# Global variable to hold the current method's name.", 
      "        ", 
      "def getClusterHistogram(pointsInCluster, trueLabelHash, histSize = 101):", 
      "    '''", 
      "    Given all the points in a cluster, extract a histogram which plots the ", 
      "    amount of those points allocated to a true label of the data. This function", 
      "    is used as a degree of approximation to cluster accuracy.", 
      "    ", 
      "    @param pointsInCluster: 2D ndarray which holds all the D-dimensional points assigned to the cluster.", 
      "    @param trueLabelHash: a hash table which maps the index of a point to its true label in the data.", 
      "    @param histSize: The size of the histogram to build (by default 101, corresponding to Caltech 101's labels).", 
      "    @return a histogram as described above", 
      "    @return the maximal true label in the cluster", 
      "    @raise LogicalError when the first or second parameter is None, or when histSize <= 0.      ", 
      "    '''", 
      "    ", 
      "    if pointsInCluster is None or trueLabelHash is None:", 
      "        raise LogicalError, \"Method %s: None arguments were provided.\" %(stack()[0][3])", 
      "    if histSize is None or histSize <=0:", 
      "        raise LogicalError, \"Method %s: histSize parameter should be a positive integer (provided: %s)\" %(stack()[0][3], str(histSize))", 
      "    ", 
      "    trueLabels = [trueLabelHash[point] for point in pointsInCluster]", 
      "    histogram, _binEdges = np.histogram(trueLabels, range(0, histSize + 1))", 
      "    return histogram, Counter(trueLabels).most_common(1)[0][0]", 
      "", 
      "def evaluateClustering(centroids, data, assignments, trueLabelMeans, trueLabelHash, histSize=101):", 
      "    ", 
      "    '''", 
      "    Evaluates a clustering algorithm, when the true labels of the data have been given. Those", 
      "    labels are contained as mapped values in \"trueLabelHash\". ", 
      "    ", 
      "    To evaluate the clustering algorithm's accuracy, we will follow two base approaches. We first", 
      "    observe that it is possible to compute the distance of every centroid to the mean values of the ", 
      "    true labels. Therefore, for every cluster it is possible to find the category mean to which it is closest in vector space.: ", 
      "    ", 
      "    Approach #1: We will associate each centroid with its closest label and therefore compute the clustering", 
      "    quality in terms of misclassification error. In this case, the predicted labels are the clusters that ", 
      "    examples are assigned to.", 
      "    ", 
      "    Approach #2: For every cluster, we build a histogram which plots the distribution of its points over ", 
      "    the ***true*** labels. Clusters whose points' majority true label coincide with the label whose mean ", 
      "    is closest to the centroid are more \"accurate\" than ones for which this condition does not hold.", 
      "    ", 
      "    @param centroids: K x D ndarray, representing K centroids in D-space.", 
      "    @param data: N x D ndarray, representing the training data X.", 
      "    @param assignments: N-sized ndarray, mapping each example to its cluter. assignments[i] = k means that", 
      "            the ith example in \"data\" is mapped to the cluster represented by the kth centroid.", 
      "    @param trueLabelMeans: |labels| xD ndarray, holding the D-dimensional mean values of every class", 
      "    @param trueLabelHash: A hash which maps example indices to their true label.", 
      "    @param histSize: integer which represents the size of the histogram to pass to \"getClusterHistogram\".", 
      "            By default it's equal to 101, the amount of labels in Caltech101.", 
      "    @raise LogicalError: For various cases which have to do with argument sanity checking.", 
      "    @raise DatasetError: If provided with no data.", 
      "    @return The number of \"accurate\" clusters, as defined above.", 
      "    '''", 
      "    ", 
      "    if centroids is None or assignments is None or trueLabelMeans is None or trueLabelHash is None:", 
      "        raise LogicalError, \"Method %s: \\\"None\\\" argument(s) provided.\" %(stack()[0][3])", 
      "    if data is None or data.shape[0] == 0 or data.shape[1] == 0:", 
      "        raise DatasetError, \"Method %s: No training data provided.\" %(stack()[0][3])", 
      "    if histSize is None or histSize <= 0:", 
      "        raise LogicalError, \"Method %s: histSize parameter should be a positive integer (provided: %s).\" %(stack()[0][3], str(histSize))", 
      "    ", 
      "    if len(trueLabelMeans) != histSize:", 
      "        raise LogicalError, \"Method %s: trueLabelMeans array should have as many rows as true labels.\" %(stack()[0][3])", 
      "    ", 
      "    # for each centroid, find the category mean it is closest to. Then associate this cluster with this", 
      "    # mean in a hash.", 
      "     ", 
      "     ", 
      "    # I have tried quite a bit to find an efficient solution to this, and have failed. Instead,", 
      "    # I will write an inefficient for loop - based implementation.", 
      "     ", 
      "    # Careful: the trueLabelMeans 2D ndarray is zero-indexed, whereas the labels are not!", 
      "     ", 
      "    closestLabel = dict()", 
      "    for i in range(len(centroids)):", 
      "        closestLabel[i] = np.array([norm(centroids[i] - mean) for mean in trueLabelMeans]).argmin() + 1", 
      "       ", 
      "    ", 
      "    # Implement approach #1: Assuming that every assigned cluster is a predicted label, compute", 
      "    # the cluster accuracy in terms of misclassification error.", 
      "        ", 
      "    misclassifiedPoints = 0", 
      "    for exIndex in range(data.shape[0]):", 
      "        if trueLabelHash[exIndex] != closestLabel[assignments[exIndex]]:", 
      "            misclassifiedPoints+=1", 
      "    ", 
      "    errorRate = misclassifiedPoints/float(data.shape[0])", 
      "    ", 
      "    # Implement approach #2: Compute true label count histograms and gauge which clusters are \"good\". ", 
      "    # \"Good\" clusters are closest to the mean of the majority ", 
      "    # vote label voted by their points, as reported by the respective histogram.", 
      "    ", 
      "    goodCentroids = 0", 
      "    for i in range(len(centroids)):", 
      "        #print \"Examining cluster %d.\" %(i + 1)", 
      "        # Get the indices of all the points in the cluster", 
      "        pointsInCluster = [j for j in range(len(assignments)) if assignments[j] == i]", 
      "        if len(pointsInCluster) > 0:", 
      "            _clusterHist, majVoteLabel = getClusterHistogram(pointsInCluster, trueLabelHash, histSize)", 
      "            #print \"Retrieved histogram and majority vote label of cluster %d.\" %(i+1)", 
      "            if closestLabel[i] != None and majVoteLabel == closestLabel[i]:", 
      "                goodCentroids+=1 ", 
      "    ", 
      "    # Return both metrics to caller.", 
      "    ", 
      "    return errorRate, goodCentroids", 
      "</code></pre>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "The following example code cell may be used to test those functions on the SIFT data through K-means:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# This code cell allows imports to our own modules", 
      "# from within the iPython notebook.", 
      "import sys, os", 
      "sys.path.append(os.path.pardir)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 7
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "from scipy.cluster.vq import *", 
      "import pickle as pkl", 
      "import numpy as np", 
      "from unsupervised.k_means_sift import evaluateClustering", 
      "", 
      "# The SIFT data has been pre-processed already and pickled", 
      "# to disk: we therefore only have to unpickle it to memory.", 
      "fp1 = open('../proc_data/sift_data_parsed.pkl', 'rb')", 
      "fp2 = open('../proc_data/sift_data_categoryMeans.pkl', 'rb')", 
      "fp3 = open('../proc_data/sift_data_exampleHash.pkl', 'rb')", 
      "SIFTData = pkl.load(fp1)", 
      "categoryMeans = pkl.load(fp2)", 
      "exampleHash = pkl.load(fp3)", 
      "fp3.close()", 
      "fp2.close()", 
      "fp1.close()", 
      "", 
      "# K-means phase", 
      "np.random.seed() # automatically sets the random number seed to the system time", 
      "whiten(SIFTData)  # Makes all features have a variance of 1 (K-means requirement)", 
      "print \"Running K-means.\"", 
      "codebook, _distortion = kmeans(SIFTData, 101, 3)   # 3 random re-initializations, 101 clusters", 
      "assignments, _distortion = vq(SIFTData, codebook)  # vq is used to assign each example SIFTData to its closest centroid in the codebook", 
      "", 
      "# Evaluation phase", 
      "print \"Ran K-means.\"", 
      "errorRate, accurateClusters = evaluateClustering(codebook, SIFTData, assignments, categoryMeans, exampleHash, 101)", 
      "print \"Computed an error rate of %.4f%% and %d \\\"accurate\\\" clusters, which correspond to %.4f%% of total labels.\" %(100*errorRate, accurateClusters, 100*(accurateClusters/float(101)))"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "Running K-means.", 
        "Ran K-means."
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "", 
        "Computed an error rate of 68.1457% and 46 \"accurate\" clusters, which correspond to 45.5446% of total labels."
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": []
      }
     ], 
     "prompt_number": 14
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "The following code cell unpickles and displays our results after running K-means on both feature extractions with 100 random re-initializations:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "fpG1 = open('../output_data/accurateClusters_gradientFeatures.txt', 'r')", 
      "fpG2 = open('../output_data/errorRate_gradientFeatures.txt', 'r')", 
      "fpS1 = open('../output_data/accurateClusters_SIFTFeatures.txt', 'r')", 
      "fpS2 = open('../output_data/errorRate_SIFTFeatures.txt', 'r')", 
      "accClustersGradientFeats = int(fpG1.read())", 
      "errorRateGradientFeats = float(fpG2.read())", 
      "print \"After 100 random re-initializations on the gradient features, K-means produced an error rate of %.4f%% and %d \\\"accurate\\\" clusters, which correspond to %.4f%% of total labels.\" %(errorRateGradientFeats, accClustersGradientFeats, 100*(accClustersGradientFeats/float(101)))", 
      "accClustersSIFTFeats = int(fpS1.read())", 
      "errorRateSIFTFeats = float(fpS2.read())", 
      "print \"After 100 random re-initializations on the SIFT features, K-means produced an error rate of %.4f%% and %d \\\"accurate\\\" clusters, which correspond to %.4f%% of total labels.\" %(errorRateSIFTFeats, accClustersSIFTFeats, 100*(accClustersSIFTFeats/float(101)))", 
      "fpS2.close()", 
      "fpS1.close()", 
      "fpG2.close()", 
      "fpG1.close()"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "After 100 random re-initializations on the gradient features, K-means produced an error rate of 69.8312% and 51 \"accurate\" clusters, which correspond to 50.4950% of total labels.", 
        "After 100 random re-initializations on the SIFT features, K-means produced an error rate of 0.6806% and 49 \"accurate\" clusters, which correspond to 48.5149% of total labels."
       ]
      }
     ], 
     "prompt_number": 23
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "It is therefore evident that, in both versions of Caltech101, there exists much room for improvement. We hypothesize that the reason for which we did not ", 
      "see any significant improvement with the SIFT features is because SIFT, by default, does not make use of spatial information about the image locations that the descriptors", 
      "were sampled from. In addition, the unitary variance assumption that K-means makes can only serve to restrict its prediction accuracy, and we expect that", 
      "more sophisticated algorithms such as the GMM will allow for improvement."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "The following are some preliminary results that we got running the GMM on the gradient features version of Caltech 101 after 3 random re-initializations:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "import pickle as pkl", 
      "from sklearn import mixture", 
      "g1 = pkl.load(open('../proc_data/gmm_obj.pkl', 'rb')) # Trained object loaded from disk", 
      "imFeatures = pkl.load(open('../proc_data/gradient_features_traindat.pkl', 'rb'))", 
      "predLabels= g1.predict(imFeatures)", 
      "categoryMeans = pkl.load(open('../proc_data/gradient_features_categorymeans.pkl', 'rb'))", 
      "exampleHash = pkl.load(open('../proc_data/examplehash.pkl', 'rb'))", 
      "predMeans = g1.means_", 
      "closestLabel = dict()", 
      "for i in range(len(predMeans)):", 
      "    closestLabel[i] = np.array([norm(predMeans[i] - mean) for mean in categoryMeans]).argmin() + 1", 
      "misclassifiedPoints = 0", 
      "for exIndex in range(imFeatures.shape[0]):", 
      "    if exampleHash[exIndex] != closestLabel[predLabels[exIndex]]:", 
      "        misclassifiedPoints+=1", 
      "errorRate = 100*(misclassifiedPoints/float(imFeatures.shape[0]))", 
      "print \"Error rate = %.4f%%\" %(errorRate)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "Error rate = 65.5180%"
       ]
      }
     ], 
     "prompt_number": 1
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "This result improves over the performance of K-means for the same data by about 4.3%. Note that", 
      "the GMM model pickled from disk only takes the diagonal of the covariance matrix into account (see \"remaining work\" section on how we can have improvement over this)."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "####V) Remaining work", 
      "", 
      "Much work remains to be done. First of all, we need to implement GMM clustering. We began using a custom GMM implementation which turned out to be buggy", 
      "and consumed too many computational resources to be realistically applicable. We are looking towards ready-baked implementations of GMMs, such as the one", 
      "used in the scikit-learn Python library: http://scikit-learn.github.com/scikit-learn.org/dev/modules/mixture.html. This library is interesting", 
      "in that it allows us to use both a scalar variance for every label as well as a full-blown covariance matrix which describes the feature dependencies", 
      "in full. However, what we really want is to capture local dependencies in the image. For example, pixels in a square area are definitely going to be dependent.", 
      "If dependencies emerge between pixels far away from one another (maybe because they belong in a \"background\" portion of an image), we do not want to capture them,", 
      "because they might yield artificial misclassifications. The following is the formula for the computation of the fractional assignments $z_{n,k}$ (E-step) for the", 
      "GMM with partial feature correlations:", 
      "", 
      "$$", 
      "z_{n,k} = \\theta_k [ 2 \\pi ||\\Sigma_n||^2 ]^{- D/2 } \\exp [ - (\\vec x_n - \\vec \\mu_k) \\Sigma_n^{-1} (\\vec x_n - \\vec \\mu_k )]", 
      "$$", 
      "", 
      "where the covariance matrix $\\Sigma_n = \\lbrace \\sigma_{i,j} : 1 \\leq i,j \\leq 588 \\rbrace \\ \\ \\ $ can be calculated from the full covariance matrix by setting $\\sigma_{i,j} = 0 : j \\neq i, j \\neq i+1 $ ", 
      "", 
      "Finally, if time permits, we would like to compare the gradient features and the SIFT", 
      "features in a supervised setting in the task of Caltech101 image classification. We intend to use SVMs as our supervised learning algorithm and libSVM", 
      "as the relevant toolkit."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "####VI) Problems encountered and solutions introduced", 
      "", 
      "One problem that we encountered is that the cardinalities of the different versions of the Caltech101 datasets was not quite the same. In the SIFT version, there", 
      "exist 8677 images, with no splitting into training and testing. In the gradients data, there exist 4367 training images and 2323 testing images, which we", 
      "cannot possibly conjoin with the training images since they are unlabeled. Therefore, in the SIFT feature extraction case, we have about twice as much", 
      "data to estimate clusters over. ", 
      "", 
      "We attempted to solve this problem by running the SIFT evaluation over a number of different random sub-samplings of the data, with about 50% of the data", 
      "available at each sample. In the end, we simply averaged our metrics produced at every sub-sampling. We followed two different sub-sampling approaches to do this:", 
      "", 
      "1. _Brute-force sub-sampling_: We randomly permute the data and take the first half of its examples as our sample.", 
      "", 
      "2. _Class-sensitive sub-sampling_: We retain 50% of each class's examples in our new sample.", 
      "", 
      "With both sub-sampling approaches described above, we got very poor results on the SIFT data (approximately 90% error rate and never more than 11 \"accurate\" ", 
      "clusters). This is because of the following reasons:", 
      "", 
      "1. In the first approach, many classes might be almost completely swept away from the picture, with very few points left to represent them", 
      "in the clustering process. Those points are very likely to be pulled to the wrong cluster in vector space, exactly because of their low numbers.", 
      "", 
      "2. The second approach was also affected because, in Caltech101, class cardinalities are very unbalanced. For instance, in the gradient data, there exist 399 images of aeroplanes and 19 images of ", 
      "wrenches. Further lowering down the number of examples assigned to the unprivileged classes will make them practically obsolete during clustering; ", 
      "more populous clusters will absorve the small number of points assigned to them.", 
      "", 
      "Therefore, we ended up simply running K-means on the entirety of the SIFT dataset (after 100 random re-initializations), without doing sub-sampling.", 
      "", 
      "As mentioned earlier, another problem that we have encountered is the application of the GMM algorithm on our data. To solve this problem, we will try to", 
      "run the scikit-learn library's GMM class on our data and, if that doesn't work, we will resort to some other library."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "####VII) Problems we foresee and proposed solutions", 
      "", 
      "- We need to improve our evaluation method. Currently we are heavily based upon the notion of proximity between D-dimensional means. After some input from the ", 
      "instructor we realized that there might be better ways to evaluate our clusters, such as measuring the entropy of every cluster true label histogram (as described", 
      "in the \"Progress so far\" section)."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "####VII) Acknowledgements and References", 
      "", 
      "We would not have been able to make this progress without significant help from both the instructor and the course TA Mohammad Rastegari. Additionally,", 
      "we are greatful to Drs. Sebastian Nowozin and Peter Gehler, who answered questions about the SIFT representation of Caltech 101 which they had arranged for their ", 
      "relevant IICV '09 paper (http://files.is.tue.mpg.de/pgehler/projects/iccv09/)."
     ]
    }
   ]
  }
 ]
}