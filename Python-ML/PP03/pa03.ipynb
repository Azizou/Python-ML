{
 "metadata": {
  "name": "pa03"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown", 
     "source": [
      "#PA03: Ensemble Learning (Due Nov 16)", 
      "", 
      "In this programming assignment you will implement and experiment with learning ensembles in recommender systems. "
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "##The MovieLens Dataset", 
      "", 
      "We are using the movielens dataset obtained here http://www.grouplens.org/node/73. The dataset consists", 
      "of 100k ratings by ~900 users on ~1500 items. The idea is to predict user ratings on a given item, ", 
      "so each (user,item) pair is an example, features include user and item features. ", 
      "", 
      "We provide a training dataset of ~90,000 movie recommendations for your use in this assignment. You can look at file", 
      "`pa03/util/prep_data.py` to see the code that was used to create the dataset. The datatable is a pickled pandas", 
      "`DataFrame` located at `movie_data/ratings_train.pda`. We also provide a test dataset of ~9500 ratings in file", 
      "`movie_data/ratings_test.pda`."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Let's load the data and get do some exploration"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# This code cell makes sure that certain import statements which occur", 
      "# in files themselves imported by code cells within this notebook work.", 
      "import sys, os", 
      "sys.path.append(os.getcwd()+ \"/code\")"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 1
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "import pandas as pd", 
      "ratings=pd.load('input_data/ratings_train.pda')", 
      "ratings = ratings.dropna() # get rid of 8 entries which have null decade values", 
      "ratings.index = range(0, ratings.index.size)", 
      "nratings = ratings.shape[0]", 
      "print 'there are %d ratings' % nratings", 
      "print ratings.columns"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 2
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "The columns are fairly self-explanatory:", 
      "", 
      "* rating: a rating from 1-5  ", 
      "* age: user's age (numeric)  ", 
      "* gender: user's gender (string, \"M\" or \"F\")  ", 
      "* occupation: user's occupation (string)  ", 
      "* Action,...,Western: binary (0,1) variables indicating movie genre  ", 
      "* decade: decade in which movies were released (numeric e.g., 90)  ", 
      "* isgood: binary (+1,-1) variable indicating if rating > 3"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Question 1: Fill in the code below to get some information about the dataset"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "# example: get a histogram of the number of ratings per age group in the dataset", 
      "figure()", 
      "ratings.groupby('age').size().plot(kind='bar')", 
      "title('Amount of ratings per age')", 
      "show()", 
      "", 
      "# example: how many ratings are there per gender?", 
      "figure()", 
      "ratings.groupby('gender').size().plot(kind='bar')", 
      "title('Amount of ratings per gender')", 
      "show()", 
      "", 
      "# COMPLETE: how many ratings are there per occupation?", 
      "# RESPONSE: ", 
      "figure()", 
      "ratings.groupby('occupation').size().plot(kind='bar')", 
      "title('Amount of ratings per occupation')", 
      "show()", 
      "", 
      "# COMPLETE: how many ratings are there per decade?", 
      "# RESPONSE: ", 
      "figure()", 
      "ratings.groupby('decade').size().plot(kind='bar')", 
      "title('Amount of ratings per movie release decade')", 
      "show()", 
      "", 
      "# COMPLETE: what is the number of good movies and bad movies (isgood==+1, isgood==-1)?", 
      "# RESPONSE: ", 
      "", 
      "figure()", 
      "ratings.groupby('isgood').size().plot(kind='bar')", 
      "title('Amount of \"good\" (+1) and \"bad\" (-1) movie ratings.')", 
      "show()", 
      "", 
      "# COMPLETE: how many examples per rating value (1-5) are there?", 
      "# RESPONSE: ", 
      "", 
      "figure()", 
      "ratings.groupby('rating').size().plot(kind='bar')", 
      "title('Amount of ratings per rating label.')", 
      "show()", 
      "", 
      "# COMPLETE: how many ratings per genre are there? (hint look at the pandas.DataFrame.apply function)", 
      "# RESPONSE: ", 
      "", 
      "# First, slice the \"ratings\" pandas.DataFrame to keep only the columns which refer to movie genres.", 
      "genres = ratings.ix[:, 'Action':'Western']", 
      "", 
      "# Second, define the function to be applied to this sliced dataframe.", 
      "def gatherSums(col):", 
      "    return np.count_nonzero(col)    # Remember that those features are binary, so we only want the counts of 1s in the columns.", 
      "", 
      "# Third, apply this function to the sliced dataframe.", 
      "# This will return a pandas.Series which will hold the counts of the ratings per genre.", 
      "counts = ratings.ix[:, 'Action':'Western'].apply(gatherSums, axis = 0)", 
      "", 
      "# Finally, plot this Series.", 
      "figure()", 
      "counts.plot(kind='bar')", 
      "title(\"Ratings per movie genre\")", 
      "show()", 
      "", 
      "    "
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 3
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "# COMPLETE: how many ratings are there for dramas from the 90's from 25-30 year old male students?", 
      "# RESPONSE: ", 
      "requiredDF = ratings[(ratings['Drama']==1) & (ratings['decade']==90) & (ratings['gender']==\"M\") & (ratings['age'] == \"(25, 30]\")]", 
      "print \"Number of required ratings: %d\" % len(requiredDF) ", 
      "", 
      "# COMPLETE: what is the proportion of good movies (isgood == +1) for these ratings", 
      "# RESPONSE:", 
      "from __future__ import division", 
      "numIsGood = len(requiredDF[requiredDF['isgood'] == 1])", 
      "print 'Among those, %.2f%%  were \\\"good\\\" ratings.' % (100*numIsGood / len(requiredDF))"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 4
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "##Collaborative Filtering", 
      "", 
      "Modern recommender systems use both information about users and items (content-based system) along", 
      "with previous ratings. The challenge is how to incorporate previous ratings into the recommender system.", 
      "As an example, suppose you want to predict if user `Joe` will rate movie `Weekend at Bernie's` as a good", 
      "movie. One way is to see how all other users have rated `Weekend at Bernie's` and use those rankings in your ", 
      "classifier (for example, by using the movie's mean rating). ", 
      "", 
      "A slightly more sophisticated way of doing it is to weight each user's rating by how similarly they rate movies ", 
      "to `Joe`. To implement this we need to define user similarities. One way of doing this is to represent each", 
      "user by the vector of ratings they have made on movies and use the ", 
      "cosine between these vectors as the similarity.", 
      "For instance, if the ratings for user `Joe` is denoted as $x$ and the ratings for user `Sandy` is denoted as ", 
      "$y$, then the similarity between `Joe` and `Sandy` is", 
      "", 
      "$$", 
      "w_{x,y}=\\frac{x \\cdot y}{\\|x\\|_2 \\|y\\|_2}", 
      "$$", 
      "", 
      "To determine `Joe's` rating for the movie, we find the most similar users to `Joe`, say 20 of them,", 
      "and calculate the average rating for `Weekend at Bernies` for these users, weighted by each user's similarity", 
      "to `Joe`. ", 
      "", 
      "We have provided code to calculate user similarities and provide similarity weighted ratings."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "from code.util.cfiltering import CFilter", 
      "", 
      "# create object on the first 10,000 ratings (you may want to build this on the complete training set", 
      "# of ratings to answer the question below).", 
      "", 
      "cf=CFilter(ratings.ix[:10000,:])   ", 
      "", 
      "# Predict user-user collaborative filtering ratings for all the training data.  ", 
      "cf_ratings=cf.get_user_cf_rating(ratings)", 
      "", 
      "# print the first 10", 
      "print cf_ratings[:10]"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 6
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Question 2: How good are these predicted ratings? How well can you predict if you get a good rating using this? (I am asking about training set ", 
      "accuracy here, you'll answer questions about generalization error later).", 
      "", 
      "###Response: ", 
      "", 
      "There are two ways to think of this problem. First, it should be obvious that it bears many similarities to traditional ranking problems. So, we could", 
      "treat it as a regression problem, except for the fact that instead of predicting *any* real value, we predict a real value in the interval [0, 5]. Because", 
      "we want to answer the question: \"How much, on average, was the collaborative filtering rating \"off\" of the real rating of the movie\", we will use *average", 
      "loss* instead of squared loss (this will take some time becuase we are building the object on all of our ratings' data):"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "# We will now build a CFilter object on all ratings", 
      "cfUser=CFilter(ratings)", 
      "cfUser_ratings = cfUser.get_user_cf_rating(ratings)", 
      "avgRatingDeviation = np.sum(np.abs(cfUser_ratings - ratings['rating'])) / len(ratings)", 
      "print avgRatingDeviation"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 7
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "So the average deviation from the training set rankings was about 0.86 points. However, this does not tell the entire story. This is because", 
      "there exists the binary feature \"isgood\", whose value changes around the rating value of 3. Consider, for instance, a movie", 
      "whose true rating was 3. On average, our rating is \"off\" by about 0.86, so we can say in this case that we will either predict 2.24 or 3.86. However, whether", 
      "we predict 2.24 or 3.86 greatly impacts the \"isgood\" column: if we predict the second rating, we will have *misclassified*  the movie as being \"good\".", 
      "", 
      "We are therefore compelled to also measure the *average squared loss* introduced by our collaborative filtering with respect to the classification", 
      "of movies as \"good\" or \"bad\":"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "userCollaborativeLabels = np.array([1 if rat > 3 else 0 for rat in cfUser_ratings])", 
      "# make the \"isgood\" column of the data map to {0,1} instead of {-1, 1}", 
      "# so that average squared loss works correctly", 
      "trueLabels = np.array([1 if lab == 1 else 0 for lab in ratings['isgood']])", 
      "", 
      "# now compute averaged squared error between those two arrays", 
      "avgSqLoss = np.sum(np.square(userCollaborativeLabels - trueLabels)) / len(ratings)", 
      "print \"Average squared loss introduced by user-user collaborative filtering: \" + str(avgSqLoss)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 8
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "So a deviation of about 0.86 points from the true rating yields a (significant) training error of about 35%."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Question 3: Extend the code for collaborative filtering in file `code.util.cfiltering.py` to implement item-item similarity (e.g., compute", 
      "`Joe's` rating for `Weekend at Bernie's` from `Joe's` ratings for similar movies, again using cosine similarity). ", 
      "", 
      "###Response:", 
      "", 
      "We followed two approaches to respond to this question. First, we expressed a movie as a vector of all its possible genres, as well as the", 
      "decade in which it was released. This was motivated by the fact that a good portion of MovieLens movies have been annotated as belonging", 
      "to more than one movie genre, as is proven below:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "horizontalCounts = ratings.ix[:, 'Action':'Western'].apply(gatherSums, axis = 1)   # applying function per row", 
      "print \"In the MovieLens dataset, there exist %d movies that have been annotated as belonging to more than 1 genre.\" %(len(horizontalCounts[horizontalCounts > 1]))", 
      "print \"This corresponds to %.2f%% of movies.\" %(100*len(horizontalCounts[horizontalCounts > 1])/nratings)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 9
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "To implement item-item rating with this movie representation, we modified the provided code to create the Python module \"cfiltering_item2.py\" in package \"code.util\". Similarly", 
      "to the provided implementation, we have defined a class (CFilter_Item) which contains a constructor, a stringifier and a ", 
      "\"get_cf_rating\" method that does all the hard work. The constructor is once again responsible for creating the hash tables ", 
      "from the ratings DataFrame, and it does that through methods that are in the module scope, but not in the class scope.", 
      "", 
      "The following cell contains the module code which, as already mentioned, is also available in the \"code.util.cfiltering_item2.py\" module."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "<code><pre>", 
      "from __future__ import division", 
      "'''", 
      "Created on Nov 3, 2012", 
      "", 
      "@author: Jason", 
      "'''", 
      "", 
      "\"\"\"", 
      "Extension of collaborative filtering utilities", 
      "to perform item-item collaborative filtering", 
      "", 
      "This version represents each movie by a vector", 
      "consisting of all its possible genres, as well", 
      "as the decade in which it was released.", 
      "\"\"\"", 
      "", 
      "import numpy as np", 
      "from mlExceptions import LogicalError, DatasetError", 
      "", 
      "", 
      "def make_ratings_hash(ratings):", 
      "    ", 
      "    \"\"\"", 
      "    Make a hashtable of ratings indexed by itemId and pointing to", 
      "    the vector (genres, decade) that fully characterize an item.", 
      "    \"\"\"", 
      "    ", 
      "    rhash = {}", 
      "     ", 
      "    # For every rating, check if the relevant item is already in the map.", 
      "    # If not, add it to the map. Key is item_id, mapped value is the vector", 
      "    # consisting of the possible genres and the decade of the movie.", 
      "    ", 
      "    for row_indx, itemid in ratings['itemid'].iteritems():", 
      "        if itemid not in rhash:", 
      "            itemData = ratings.ix[row_indx, 'Action' : 'decade']", 
      "            rhash[itemid] = itemData", 
      "    return rhash", 
      "", 
      "def compute_norms(items):", 
      "    ", 
      "    \"\"\"", 
      "    Compute the norms of the item vectors provided.", 
      "    ", 
      "    Arguments:", 
      "    items -- a hashmap which maps itemIDs to the characteristic vectors", 
      "    \"\"\"", 
      "    ", 
      "    norms = {}", 
      "    ", 
      "    for item in items:", 
      "        norms[item] = np.sqrt(np.sum(np.square(items[item])))", 
      "        ", 
      "    return norms", 
      "    ", 
      "def get_item_neighborhood(ratings, itemid, size, norms):", 
      "    ", 
      "    \"\"\"", 
      "    Find the nearest items and their cosine similarity to a given item", 
      "", 
      "    Arguments:", 
      "    ratings -- a hash with itemid keys and item vector values.", 
      "    item -- the id of the item whose neighborhood is being calculated", 
      "    size -- the number of items to be considered for item \"itemid\"'s neighborhood", 
      "    norms -- a named vector (pandas.Series) of item l2 norms in rating space", 
      "", 
      "    Returns:", 
      "    items -- a vector of the ids of the nearest items (the neighbors)", 
      "    weights -- a vector of cosine similarities for the neighbors", 
      "    \"\"\"", 
      "    ", 
      "    # Create a hash which will have the current itemid as the key", 
      "    # and will map to the dot products of the current item's vectorized interpretation", 
      "    # and all the different neighbor items.", 
      "    ", 
      "    similarityhash = {}", 
      "    ", 
      "    for otheritemid, otheritemvector in ratings.iteritems():                        # remember that ratings is a hash table", 
      "        if otheritemid == itemid:", 
      "            continue                                                                # doesn't make much sense to compute the distance to ourselves.", 
      "", 
      "        if otheritemid not in similarityhash:                                       # If you haven't stored the item currently considered", 
      "            similarityhash[otheritemid] = 0                                         # in your hash, do it now.", 
      "", 
      "        if len(ratings[itemid]) != len(otheritemvector):", 
      "            raise LogicalError ,\"Cannot compute the dot product of mismatch size vectors\"", 
      "        similarityhash[otheritemid] = np.dot(ratings[itemid], otheritemvector)", 
      "    ", 
      "    ", 
      "    # Now we will loop through the hash and update each value ", 
      "    # by dividing it with the product of the current item's norm", 
      "    # and the neighbor's norm. This completes computation of the cosine", 
      "    # similarities between the current movie and all other movies in", 
      "    # the database.", 
      "    ", 
      "    for otheritemid, _otheritemvector in similarityhash.iteritems():", 
      "        nx=norms[itemid]", 
      "        ny=norms[otheritemid]", 
      "        similarityhash[otheritemid] = similarityhash[otheritemid]/float(nx*ny)   # there you have it, the full cosine similarity between \"itemid\" and \"otheritemd\"", 
      "    ", 
      "    ", 
      "    # Now that we have the cosine similarities between the current", 
      "    # movie and all other movies, we will create the current movie's", 
      "    # neighborhood by considering only the \"size\" most similar movies", 
      "    # to the current movie (as dictated by the cosine similarities, i.e", 
      "    # the values of the hash).", 
      "    ", 
      "    indx = np.argsort(-np.array(similarityhash.values()))[:size]                  # find the indices that sort the hash by cosine similarity in DESCENDING (-) order", 
      "    ", 
      "    # Finally, we will return both the itemIDs of the neighborhood,", 
      "    # as well as the cosine similarities of the neighborhood.", 
      "    ", 
      "    items = np.array(similarityhash.keys())[indx]                                 # and retrieve the top 20 ones. Then, retrieve both the items (keys)", 
      "    weights = np.array(similarityhash.values())[indx]                             # and the similarities themselves", 
      "    return items, weights                                              ", 
      "", 
      "def make_neighborhood_hash(itemids, ratings, size, norms):", 
      "    ", 
      "    \"\"\" ", 
      "    Creates the neighborhood of every user in the database", 
      "    ", 
      "    Arguments:", 
      "    ", 
      "    itemids -- pandas.Series object which holds all the different item ids in the database", 
      "    ratings -- a hash with item id keys and item vector values.", 
      "    size -- an integer which dictates the maximum number of neighbors to consider.", 
      "    norms -- a hash with item id keys and float values representing the norms of the respective items.", 
      "    \"\"\"", 
      "    ", 
      "    # the following hashes will map from item_id to stuff", 
      "    ", 
      "    neighbors = {}", 
      "    weights = {}", 
      "", 
      "    for itemid in itemids:", 
      "        if itemid not in neighbors:                                         # put it in ", 
      "            res = get_item_neighborhood(ratings, itemid, size, norms)", 
      "            neighbors[itemid], weights[itemid] = res                        # now we have the neighbor items' ids and their cosine similarities to the current item id, associated with the current item id", 
      "            ", 
      "            # Some sanity checks", 
      "            ", 
      "            if len(neighbors[itemid]) != size:", 
      "                raise LogicalError ,\"Total number of neighbors of item id %s should be %d.\" %(itemid, size)", 
      "            if len(weights[itemid]) != len(neighbors[itemid]):", 
      "                raise LogicalError ,\"The number of cosine similarities considered for item id %s should be the same as its neighbors.\" %(itemid)", 
      "            ", 
      "    return neighbors, weights                                               # The neighbors of every item, along with their associated cosine similarity to the item, are returned.", 
      "", 
      "def extractRating(rater_id, movie_id, ratings):", 
      "    ", 
      "    \"\"\"", 
      "    A method that extracts the rating at one position of the data. ", 
      "    If the rating is non-existent, it returns zero.", 
      "    ", 
      "    Arguments: ", 
      "    rater_id -- integer representing the rater id", 
      "    movie_id -- integer represeting the movie id. Along with rater id, it uniquely characterizes an enty", 
      "    ratings -- pandas.Dataframe which holds our ratings data.", 
      "    ", 
      "    Returns:", 
      "    The relevant rating, or 0 if it doesn't exist. ", 
      "    \"\"\"", 
      "    ", 
      "    # Sanity checking first", 
      "    ", 
      "    if len(ratings[ratings['userid'] == rater_id]) == 0:", 
      "        raise LogicalError ,\"No ratings detected for user: %s.\" %(rater_id)", 
      "    ", 
      "    ", 
      "    # Find all ratings of the user provided, and then constrain those", 
      "    # to the single rating corresponding to the (rater_id, movie_id) pair.", 
      "    ratingsOfUser = ratings[ratings['userid'] == rater_id]", 
      "    specificRating = ratingsOfUser[ratingsOfUser['itemid'] == movie_id]['rating']", 
      "    ", 
      "    # Return 0 if there exists no rating, or actual rating otherwise", 
      "    ", 
      "    return 0 if specificRating.values.size == 0 else specificRating.values[0]", 
      "", 
      "class CFilter_item2(object):", 
      "    ", 
      "    \"\"\"", 
      "    A class to get ratings from item-item collaborative filtering", 
      "    \"\"\"", 
      "", 
      "    def __init__(self, ratings, size=20):", 
      "        ", 
      "        \"\"\"", 
      "        Arguments:", 
      "        ratings -- a pandas.DataFrame of movie ratings (see prep_data)", 
      "        size -- item neighborhood size (default=20)", 
      "        \"\"\"", 
      "        ", 
      "        self.size = size", 
      "        self.itemHash = make_ratings_hash(ratings)      # self.itemHash is now a hash with item_id as the keys ", 
      "                                                        # and [Action, ...., Western, decade] as the values", 
      "        ", 
      "        # Compute the two-norm for every item's representative vector, ", 
      "        # since we need it for the cosine similarity", 
      "        norms = compute_norms(self.itemHash)                  # norms is now a hash that maps item_id to norm of item_id", 
      "        itemids = ratings['itemid']                         # note that this is not a unique Series.", 
      "        self.neighbors, self.weights = make_neighborhood_hash(itemids, self.itemHash, size, norms)  ", 
      "", 
      "    def __repr__(self):", 
      "        return 'CFilter_item which implements item-item collaborative filtering, with %d ratings for %d items' % (len(self.ratings), len(self.neighbors))", 
      "", 
      "    def get_item_2_cf_rating(self, ratings):", 
      "        ", 
      "        \"\"\"", 
      "        Get item ratings from item neighborhood", 
      "", 
      "        Arguments:", 
      "        ratings -- a pandas.DataFrame of movie ratings (see prep_data)", 
      "", 
      "        Returns:", 
      "        A numpy array of collaborative filter item ratings from item neighborhoods. If itemid is not in", 
      "        database, 0 is returned as the item rating. Ratings are discretized from 0-5 in 0.25 increments.", 
      "", 
      "        \"\"\"", 
      "        nratings = ratings.shape[0]", 
      "        if nratings == 0:", 
      "            raise DatasetError ,\"Empty dataset provided.\"", 
      "        cf_rating=np.zeros(nratings)                                        # As the description mentions: \"If itemid is not in database...\" ", 
      "        for itemid in self.neighbors.keys():                                # For every item id in the database", 
      "            indx = ratings['itemid']==itemid                                # indx holds indices of the ratings of the current item examined. ", 
      "            if np.sum(indx)==0:                                             # No ratings found for this movie. Continue on to the next movie.", 
      "                continue                                                    # this is consistent with the fact that we want a movie without ratings to be collaboratively rated as \"zero\"", 
      "", 
      "            users_who_rated = ratings['userid'][indx].values                # store the ids of the users who rated the current item", 
      "            m = np.sum(indx)                                                # m: number of ratings of this item.", 
      "            n = len(self.neighbors[itemid])                                 # n: number of neighbors of this item.", 
      "", 
      "            nratings=np.zeros((m,n))                                        # m x n ndarray holding the collaborative ratings of the current item.", 
      "            w = np.zeros((m,n))                                             # same, but this time this array holds weights (cosine similarities)", 
      "", 
      "            for i in xrange(m):                                             # for every rating of the currently examined movie", 
      "                currentRater = users_who_rated[i]                           # fetch the rater", 
      "                for j in xrange(n):                                         # for every neighbor item of the item \"itemid\"", 
      "                    otheritemid = self.neighbors[itemid][j]                 # get its id", 
      "                    nratings[i, j] = extractRating(currentRater, ", 
      "                                                   otheritemid, ratings)    # store the specific rating corresponding to (currentRater, otheritemid) (or 0 if it doesn't exist)", 
      "                    # Sanity check: is rating within bounds?", 
      "                    if nratings[i, j] < 0:", 
      "                        raise LogicalError, \" for userid %d and movieid %d, extractRating returned a negative rating.\" %(currentRater, otheritemid)", 
      "                    if nratings[i, j] > 5:", 
      "                        raise LogicalError, \" for userid %d and movieid %d, extractRating returned a rating above 5.\" %(currentRater, otheritemid) ", 
      "                    w[i,j] = self.weights[itemid][j]                        # and the weight (cosine similarity between the current user and the neighbor)", 
      "                #end for", 
      "            #end for", 
      "              ", 
      "            sw = np.sum(w,axis=1)                                           # sw (sum of w): 1D array that maintains the SUM of the consine similarities of the current item and every item in its neighborhood", 
      "            ", 
      "            # The following three lines of code are not useful for us ", 
      "            # in this item-item collaborative filtering modification ", 
      "            # of the original code, because the presence of the movie release", 
      "            # decade in the vector representing each movie ensures that the dot", 
      "            # product will always be > 0, so there will be no non-zero sums. ", 
      "            # However, out of pure fear that maybe we haven't thought something through, ", 
      "            # we will keep all references to the vector \"keep\". No pun intended,", 
      "            # but nobody cares whether our pun was intended anyway.", 
      "            ", 
      "            keep = sw>0                                                     ", 
      "            if np.sum(keep)==0:                                             ", 
      "                continue                                                    ", 
      "", 
      "            nratings *= w                                                   # Multiply the neighbors' ratings by the neighbors' cosine similarities. This multiplies the matrices element-wise.", 
      "            res = np.sum(nratings,axis=1)                                   # For every rating of the current movie, retrieve the sum of the weighted ratings of the neighbor movies.", 
      "", 
      "            res[keep.nonzero()] /= sw[keep.nonzero()]                       # Normalize by the sum of cosine similarities of all neighbors.", 
      "            cf_rating[indx] = res                                           # For every item rated, we now also have the collaborative filtering rating of the neighborhood.", 
      "            ", 
      "        # end for item_id", 
      "        ", 
      "        return cf_rating", 
      "</pre></code>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "As will be seen in our response to question 4, we were rather dissatisfied with our training accuracy introduced by this form of movie similarity,", 
      "so we also created some code to compute movie similarities the same way user similarities are computed, i.e, by defining two movies to be similar if ", 
      "they have had, roughly, the same ratings overall. The relevant code is available in the Python module \"code.util.cfiltering_item.py\", and because it is", 
      "very similar to the provided user-user collaborative filtering code, we choose to note \"pollute\" the current notebook with it."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Question 4: Are predicted ratings from item-item similarity better?", 
      "", 
      "###Response:", 
      "", 
      "Given our differing implementations of item-item similarity, we have two responses to this question. The first response will be given by retrieving", 
      "the collaborative item-item ratings in the case where movie similarities are gauged by the genre + decade vectorized representation. The second response", 
      "will be given by retrieving the collaborative ratings in the case where movies are represented by the vectors of ratings made for them. As we will see,", 
      "the results are markedly different.", 
      "", 
      "", 
      "### Case 1: Movies represented by their genres and decade.", 
      "", 
      "After defining the item-item collaborative filtering, we ran a small main() method to store the generated item-item collaborative ratings on disk, ", 
      "because they took significant time to be computed and we do not wish to run heavy computations on this notebook.", 
      "Therefore, to respond to the query, we will load the ratings from disk, and compute ", 
      "", 
      "(i) Average deviation from the actual rating, and ", 
      "", 
      "(ii) Average Squared Loss for the \"isgood\" column."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "import pickle as pkl", 
      "fp = open('proc_data/item_item_ratings_firstVersion.pda', 'rb')", 
      "item_item_ratings = pkl.load(fp)", 
      "fp.close()", 
      "", 
      "# (i) Average deviation from true rating", 
      "", 
      "avgRatingDeviation = np.sum(np.abs(item_item_ratings - ratings['rating'])) / len(ratings)", 
      "print \"On average, the rankings of our first version of item-item collaborative filtering were off by: %.3f\" %(avgRatingDeviation) ", 
      "", 
      "# (ii) Average misclassification squared loss", 
      "", 
      "itemCollaborativeLabels = np.array([1 if rat > 3 else 0 for rat in item_item_ratings])", 
      "# make the \"isgood\" column of the data map to {0,1} instead of {-1, 1}", 
      "trueLabels = np.array([1 if lab == 1 else 0 for lab in ratings['isgood']])", 
      "avgSqLoss = np.sum(np.square(itemCollaborativeLabels - trueLabels)) / len(ratings)", 
      "print \"Average squared loss introduced by this version of item-item collaborative filtering: %.6f\" %(avgSqLoss)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 10
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "As can be witnessed, this version of the item-item collaborative rating is, on average, wrong by almost 3 points! In addition, the training", 
      "error is increased to about 55%. This is definitely worse than the user-user collaborative rating.", 
      "", 
      "Let's see whether we can improve upon this by introducing our second case of item-item collaborative filtering, which is simply \"mirrorring\" the user-user collaborative ", 
      "filtering ratings.", 
      "", 
      "###Case 2: Movies represented by the ratings made upon them.", 
      "", 
      "Despite the fact that rating retrieval is relevantly fast in this case (as fast as the user-user collaborative filtering case), we have once ", 
      "again stored the ratings to disk to allow for quick access."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "fp = open('proc_data/item_item_ratings_secondVersion.pda', 'rb')", 
      "item_item_ratings_2 = pkl.load(fp)", 
      "fp.close()", 
      "", 
      "# (i) Average deviation from true rating", 
      "avgRatingDeviation = np.sum(np.abs(item_item_ratings_2 - ratings['rating'])) / len(ratings)", 
      "print \"On average, the rankings of our second version of item-item collaborative filtering were off by: %.3f\" %(avgRatingDeviation) ", 
      "", 
      "# (ii) Average misclassification squared loss", 
      "", 
      "itemCollaborativeLabels = np.array([1 if rat > 3 else 0 for rat in item_item_ratings_2])", 
      "# make the \"isgood\" column of the data map to {0,1} instead of {-1, 1}", 
      "trueLabels = np.array([1 if lab == 1 else 0 for lab in ratings['isgood']])", 
      "avgSqLoss = np.sum(np.square(itemCollaborativeLabels - trueLabels)) / len(ratings)", 
      "print \"Average squared loss introduced by this version of item-item collaborative filtering: %f\" %(avgSqLoss)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 11
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "It is therefore made clear that, despite the decrease on average deviation from the true rating from about 0.86 to about 0.77, training error with respect", 
      "to whether a movie is correctly classified as good or bad has actually slightly increased, from 35.3% to 35.8%. Given the large improvement over the first version", 
      "of item-item collaborative filtering rating, we will stick to this version of item-item collaborative filtering for our tests on the testing data, merely", 
      "because we have to assume that both data are drawn from the same, unknown, distribution and we thus expect a better test data performance."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "##Hybrid recommender systems", 
      "", 
      "We used the following code cells in order to prepare both our training and testing data for prediction and store it on disk. "
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# We will first retrieve two collaborative filtering objects that we have stored on disk,", 
      "# one for user-user collaborative filtering and one more for item-item collaborative filtering.", 
      "# We need to do this because we will have to use these objects to get predictions for testing data.", 
      "import pickle as pkl", 
      "fp1 = open('proc_data/cf_user_object.pda', 'rb')", 
      "fp2 = open('proc_data/cf_item_object.pda', 'rb')", 
      "", 
      "cfUser_object = pkl.load(fp1)", 
      "cfItem_object = pkl.load(fp2)", 
      "", 
      "print str(cfUser_object)", 
      "print str(cfItem_object)", 
      "fp1.close()", 
      "fp2.close()"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 20
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# remove the userid, itemid and rating column from the training set", 
      "ratings.pop('userid')", 
      "ratings.pop('itemid')", 
      "_dont_print = ratings.pop('rating')"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 27
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# add discretized predicted ratings from CFilter", 
      "# discretization is from 0-5 in .25 steps", 
      "ratings['cf_user_rating']=pd.cut(cfUser_ratings,np.arange(-1,6,.25))", 
      "ratings['cf_item_rating']=pd.cut(item_item_ratings_2,np.arange(-1,6,.25))"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 28
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# now ratings is ready for prediction. We will store it on disk, to speedup future access", 
      "fp = open('proc_data/prediction_ready_trainDat.pda', 'wb')", 
      "pkl.dump(ratings, fp)", 
      "fp.close()"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 29
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# repeat the procedure of the three previous cells for the testng data", 
      "test_ratings=pd.load('input_data/ratings_test.pda')", 
      "", 
      "# add ratings to the test set using the cf objects built on the training set", 
      "test_ratings['cf_user_rating']=pd.cut(cfUser_object.get_user_cf_rating(test_ratings), np.arange(-1,6,.25))", 
      "test_ratings['cf_item_rating']=pd.cut(cfItem_object.get_item_cf_rating(test_ratings), np.arange(-1,6,.25))", 
      "# remove columns not used in 'isgood' prediction", 
      "test_ratings.pop('userid')", 
      "test_ratings.pop('itemid')", 
      "_dont_print=test_ratings.pop('rating')"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 30
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# Once again, we will store the prediction-ready testing data on disk.", 
      "fp = open('proc_data/prediction_ready_testDat.pda', 'wb')", 
      "pkl.dump(test_ratings, fp)", 
      "fp.close()"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 31
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Question 5: Compare the following versions of your recommender system:  ", 
      "", 
      " 1. content-based rec system (using only user and item features)  ", 
      " 2. using only predicted ratings from item-item similarities  ", 
      " 3. using only predicted ratings from user-user similarities  ", 
      " 4. using only predicted ratings from both item-item and user-user similarities  ", 
      " 5. content-based + item-item predicted ratings  ", 
      " 6. content-based + user-user predicted ratings  ", 
      " 7. content-based + item-item + user-user predicted ratings  ", 
      "", 
      "Note 1: You need to choose a classifier to run this test. We have provided a decision tree learner in module", 
      "`pa03.dectree.DTree`. It only handles categorical data (recall all features in this dataset are categorical),", 
      "and uses the Gini index for scoring splits. You can use this or your decision tree code from PA01.", 
      "", 
      "Note 2: Implement the bootstraped F-score metric from Algorithm 20 (pg. 65) of the book and use it to perform this comparison (i.e., provide confidence", 
      "intervals).", 
      "", 
      "Note 3: You should make the bootstrap sampling code usable beyond this question as you will need it again when implementing Bagging and Random Forest ensembles."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "###Response", 
      "", 
      "In order to respond to each and every sub-question, we will need to apply the tree to different sets of features each time. For every sub-question, \"isgood\"", 
      "is always the required label. We have used the provided decision tree code, and we alternated between maximum depths of 10 and 20.", 
      "", 
      "We have written code to compute the boostrapped F-score over K - many folds of the test data. This code can be found ", 
      "in Python module code.util.bootstrapEval.py and is also shown in the next MarkDown cell. Of note is our implementation of the `bootstrapEval` function.", 
      "This function can be called for both the context of bootstrap evaluation (computation of a bootstrapped F-score over a number of testing data folds) *and* ", 
      "the context of bagging, where the goal is to return a set of classifiers, all trained on a different folds of the training data."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "<code><pre>", 
      "from __future__ import division", 
      "'''", 
      "Created on Nov 12, 2012", 
      "", 
      "@author: jason", 
      "", 
      "A set of functions that jointly implement bootstrapped F-score and", 
      "ensemble learning with bagging of decision trees (random or non-random).", 
      "'''", 
      "", 
      "import numpy as np", 
      "import pandas as pd", 
      "import pickle as pkl", 
      "import inspect", 
      "from mlExceptions import DatasetError, LogicalError", 
      "from dectree.DTree import get_tree", 
      "", 
      "def bootStrapEval(trueLabs, predictedLabs, K, bag = False, dataset = None):", 
      "    ", 
      "    '''", 
      "    This method has two different usages, which are disambiguated between by its boolean", 
      "    4th argument, \"bag\".", 
      "     ", 
      "        1) If bag = false (the default), bootstrap evaluation is implemented. The method takes", 
      "        K different bootstrapped samples of its first two arguments (true and predicted labels)", 
      "        and computes the F-score for each sample, which it then stores in a list. At the end of", 
      "        the execution, it returns the mean and the variance of those F-scores. This is an encoding", 
      "        of algorithm 10 at page 65 of CIML.", 
      "    ", 
      "        2) If bag = true, ensemble learning via bagging is implemented. This effectively draws", 
      "        K bootstrapped samples (i.e samples with replacement) from the dataset pointed to by ", 
      "        the 5th argument and for each sample it trains a decision tree classifier. Every tree", 
      "        is stored in a list, which is returned to the caller at the end of execution.", 
      "    ", 
      "     ", 
      "    ", 
      "    @param trueLabs: If bag == false, a list of length N representing the true labels of our data. None otherwise.", 
      "    @param predictedLabs: If bag == false, a list of length N representing the predicted labels of our data. None otherwise.", 
      "    @param K: If bag == false, the number of folds to perform over the labels. Otherwise, the number of ", 
      "            bootstrapped samples to draw from the training data.", 
      "    @param bag: boolean flag. If false (the default), the method performs bootstrap resampling. If true,", 
      "            the method performs bagging of decision trees.", 
      "    @param dataset: by default, None. If bag == true,  must be non-None (this is checked for), and is ", 
      "            a reference to a pandas.DataFrame which holds the training data to draw samples from.", 
      "    @return: If bag == false, mean and standard deviation of \"K\" - many F-scores. Otherwise, list of", 
      "            trained decision tree classifiers.", 
      "    @raise LogicalError: If there is some inconsistency, among numerous possible, with respect to the", 
      "            arguments provided in each case.", 
      "    '''", 
      "    ", 
      "    # Because this method is quite complex, we need to make sure that ", 
      "    # the arguments provided to it are consistent with the context ", 
      "    # in which we want to use it. We therefore need to do some", 
      "    # sanity checking.", 
      "    ", 
      "    if K == None or K == 0: # this is applicable in both usage contexts: we need K > 0", 
      "        raise LogicalError, \"Method %s: Please provide a positive integer for the K parameter.\" % inspect.stack()[0][3]", 
      "    ", 
      "    if bag== False: # need to check the validity of the two first arguments", 
      "        if trueLabs == None or predictedLabs == None or len(trueLabs) == 0 or len(predictedLabs) == 0:", 
      "            raise LogicalError, \"Method %s: Cannot compute bootsrapped F-score without true or predicted labels.\" %  inspect.stack()[0][3]", 
      "        if len(trueLabs) != len(predictedLabs):", 
      "            raise LogicalError, \"Method %s: Mismatch between amount of true and predicted labels.\" %  inspect.stack()[0][3]", 
      "    else:   # need to check the validity of the last argument", 
      "        if dataset is None or dataset.shape[0] == 0:", 
      "            raise DatasetError, \"Method %s: Caller provided a null or empty dataset.\" % inspect.stack()[0][3]", 
      "    ", 
      "    # Case 1: Bootstrap Resampling", 
      "    ", 
      "    if bag == False:", 
      "        ", 
      "        # Initialize algorithm", 
      "    ", 
      "        scores = list()             # a list of F-scores, initially empty", 
      "        numExamples = len(trueLabs)", 
      "        ", 
      "        # For every fold", 
      "        ", 
      "        for _i in range(K):", 
      "            foldTrueLabels = list()", 
      "            foldPredictedLabels = list()", 
      "            ", 
      "            # For every example", 
      "            ", 
      "            for _j in range(numExamples):", 
      "                ", 
      "                # retrieve and store true and predicted label of example", 
      "                ", 
      "                sampledExampleIndex = np.random.randint(numExamples)        # sample a random example from 0 up to N - 1", 
      "                foldTrueLabels.append(trueLabs[sampledExampleIndex])", 
      "                foldPredictedLabels.append(predictedLabs[sampledExampleIndex])", 
      "            ", 
      "            # Compute and store the F score for the current fold.", 
      "             ", 
      "            scores.append(__computeFScore__(foldTrueLabels, foldPredictedLabels))", 
      "            ", 
      "        # Return mean and standard deviation of all F scores.", 
      "        ", 
      "        return np.mean(scores), np.std(scores)", 
      "    ", 
      "    # Case 2: Bagging of decision trees", 
      "    ", 
      "    else:", 
      "        ", 
      "        nexamples = dataset.shape[0]", 
      "        ", 
      "        # keep a list of all the decision tree classifiers ", 
      "        # that we will train", 
      "        ", 
      "        DTreeList = list()", 
      "        ", 
      "        # for every sample", 
      "        ", 
      "        for datasetSample in range(K):", 
      "            ", 
      "            # keep a list of every example that you sample.", 
      "            # In Python terms, this is a list of Series, and", 
      "            # we will convert it to a pandas.DataFrame after we", 
      "            # complete our inner loop.", 
      "            ", 
      "            examplesInSample = list()", 
      "            ", 
      "            # Select N examples for our sub-dataset", 
      "            # by sampling with replacement.", 
      "            ", 
      "            for _example in range(nexamples):", 
      "                selectedExample = np.random.randint(0, nexamples)", 
      "                examplesInSample.append(dataset.irow(selectedExample))       ", 
      "            ", 
      "            subDataset = pd.DataFrame(examplesInSample)", 
      "            subDataset.index = np.arange(subDataset.shape[0])", 
      "            ", 
      "            # Train a decision tree classifier on the bootstrapped data", 
      "            # and store it in a list.", 
      "            print \"Building tree %d.\" %(datasetSample + 1)", 
      "            tree = get_tree(subDataset, 'isgood')", 
      "            print \"Tree number %d has an optimal depth of: %d\" %(datasetSample+1, tree.optimal_depth)", 
      "            DTreeList.append(tree)", 
      "        ", 
      "        # end for _datasetSample    ", 
      "           ", 
      "        return DTreeList        ", 
      "                ", 
      "    # end function   ", 
      "    ", 
      "    ", 
      "def computeConfidenceInterval(mean, stddev, othermean):", 
      "    '''", 
      "    Computes the probability according to which the superior performance of the ", 
      "    algorithm which yielded an F-score of mean \"mean\" and standard deviation \"stddev\"", 
      "    is not due to chance. F-scores are assumed to be distributed according to ", 
      "    a gaussian distribution.", 
      "    ", 
      "    @param mean: The mean of the superior F-score.", 
      "    @param stddev: The standard deviation of the superior F-score.", 
      "    @param othermean: The inferior mean F-score.", 
      "    @return a float representing the probability that the superior performance of the ", 
      "    algorithm which yielded an F-score equal to \"mean\" is not due to chance.", 
      "    @raise LogicalError: If mean < othermean.", 
      "    '''", 
      "    ", 
      "    if mean < othermean:", 
      "        raise LogicalError, \"Method %s: The F-measure questioned is not superior to the other algorithm's F-measure.\" %(inspect.stack()[0][3])", 
      "    ", 
      "    # If mew - stdev < othermean < mew + stddev, we have a probability of 68.2%.", 
      "    if mean - stddev < othermean < mean + stddev:", 
      "        return 0.682", 
      "    # If mew - 2stddev < othermean < mew + 2stddev, we have a probability of 95.4%.", 
      "    elif mean - 2 * stddev < othermean < mean + 2 * stddev:", 
      "        return 0.954", 
      "    # If mew - 3stddev < othermean < mew + 3stddev, we have a probability of 99.7%.", 
      "    elif mean - 3 * stddev < othermean < mean + 3 * stddev:", 
      "        return 0.997", 
      "    else:", 
      "        return 1", 
      "    ", 
      "def __computeFScore__(trueLabels, predictedLabels):", 
      "    ", 
      "    '''", 
      "    Compute the F-Score of the algorithm that produced the predictedLabels argument.", 
      "    ", 
      "    @param trueLabels: a list corresponding to the true labels of a dataset. ", 
      "    @param predictedLabels: a list corresponding to the predicted labels of our learning algorithm.", 
      "    @return: the F-score computed by comparing the two labels element-wise.", 
      "    @raise  LogicalError: if the provided lists are None or empty or if the two lists are of mismatch length.", 
      "    '''", 
      "    ", 
      "    # Sanity checking", 
      "     ", 
      "    if trueLabels == None or predictedLabels == None or len(trueLabels) == 0 or len(predictedLabels) == 0:", 
      "        raise LogicalError, \"Method %s: provided empty or None lists.\" % inspect.stack()[0][3]", 
      "    if len(trueLabels) != len(predictedLabels):", 
      "        raise LogicalError, \"Method %s: mismatch length between true labels and predicted labels.\" % inspect.stack()[0][3]", 
      "    ", 
      "    ", 
      "    # Initialization of variables", 
      "    ", 
      "    TP = FP = FN = 0", 
      "    ", 
      "    # For every predicted label", 
      "    for i in range(len(predictedLabels)):", 
      "        ", 
      "        # Compare it to the true label and increment respective variable.", 
      "        if trueLabels[i] == -1 or trueLabels[i] == 0:   # Both 0 and -1 acceptable as a negative label", 
      "            if predictedLabels[i] == 1:                 # False positive", 
      "                FP+=1", 
      "        else:                                           # positive true label", 
      "            if predictedLabels[i] == 1:                 # True Positive", 
      "                TP+=1                                ", 
      "            else:                                       # False Negative", 
      "                FN+=1", 
      "    ", 
      "    # Compute Precision, Recall, F-measure, and return F-measure.", 
      "    ", 
      "    Recall = TP / (TP + FN)", 
      "    Precision = TP / (TP + FP)", 
      "    F1Score = 2 * (Recall * Precision) / (Recall + Precision)", 
      "    return F1Score ", 
      "", 
      "", 
      "", 
      "def storeFScores():", 
      "    ", 
      "    # load the prediction-ready training and testing data", 
      "        ", 
      "    trainData = pd.load('proc_data/prediction_ready_trainDat.pda')", 
      "    testData = pd.load('proc_data/prediction_ready_testDat.pda')", 
      "    trainData = trainData.dropna()", 
      "    testData = testData.dropna()", 
      "        ", 
      "    # To answer question 5, we need to perform binary classification on 7 different sub-datasets.", 
      "    # We will use Hector's decision trees, and for each case we will get both the true and the predicted ratings", 
      "    # and extract the bootstrapped F-score. As a beginning experiment, we will consider K = 10 folds", 
      "    # per bootstrap evaluation.", 
      "        ", 
      "    FScoreMeansAndVars = list()             # a list of (mean, stdev) tuples that might prove handy", 
      "        ", 
      "    # We have 7 different feature vectors for both training and testing data ", 
      "    # that we need to consider, which we now put in the following two lists.", 
      "        ", 
      "    trainFeatVecs = [", 
      "                        trainData.ix[:, :23],                                    # content-based system (user + item features)", 
      "                        trainData.ix[:, (22, 24)],                               # using only item-item predicted ratings", 
      "                        trainData.ix[:, (22, 23)],                               # using only user-user predicted ratings", 
      "                        trainData.ix[:, 22:25],                                  # using both item-item and user-user predicted ratings", 
      "                        trainData.ix[:, :23].join(trainData.ix[:, 24:25]),     # content-based + item-item predicted ratings", 
      "                        trainData.ix[:, :23].join(trainData.ix[:, 23:24]),     # content-based + user-user predicted ratings", 
      "                        trainData.ix[:, :23].join(trainData.ix[:, 23:25])      # content-based + both predicted ratings", 
      "                            ]", 
      "        ", 
      "    testFeatVecs = [", 
      "                            testData.ix[:, :23],                                     # content-based system (user + item features)", 
      "                            testData.ix[:, (22, 24)],                                # using only item-item predicted ratings", 
      "                            testData.ix[:, (22, 23)],                                # using only user-user predicted ratings", 
      "                            testData.ix[:, 22:25],                                   # using both item-item and user-user predicted ratings", 
      "                            testData.ix[:, :23].join(testData.ix[:, 24:25]),       # content-based + item-item predicted ratings", 
      "                            testData.ix[:, :23].join(testData.ix[:, 23:24]),       # content-based + user-user predicted ratings", 
      "                            testData.ix[:, :23].join(testData.ix[:, 23:25])        # content-based + both predicted ratings", 
      "                            ]", 
      "        ", 
      "        # Now that we have all 7 different training and testing datasets,", 
      "        # we can compute the bootstrapped F-score for every setup.", 
      "        # We will store these bootstrapped F-scores in a list, which we will", 
      "        # then store on disk for easy future access. We will", 
      "        # use K=100 folds for our experiments.", 
      "        ", 
      "    for i in range(len(trainFeatVecs)):", 
      "            ", 
      "        print \"Training decision tree for configuration %d.\" %(i+1)", 
      "        tree = get_tree(trainFeatVecs[i], 'isgood')", 
      "        print \"Trained a decision tree, found an optimal depth of: %d\" %(tree.optimal_depth)", 
      "        print \"Getting predictions of decision tree on testing data.\"", 
      "        predictions = tree.predict(testFeatVecs[i])", 
      "            ", 
      "        print \"Computing bootstrapped F-score.\"", 
      "        mean, stddev = bootStrapEval(testFeatVecs[i]['isgood'].values, predictions, 1000)", 
      "        print \"Computed a mean F-score of %.4f with a std. dev. of %.4f.\" %(mean, stddev)", 
      "            ", 
      "        print \"Storing bootstrapped F-score of configuration %d in a list.\" %(i+1)", 
      "        FScoreMeansAndVars.append((mean, stddev))", 
      "       ", 
      "    print \"Storing all F-scores on disk.\"", 
      "    fp = open('proc_data/bootstrappedFScores.pda', 'wb')", 
      "    pkl.dump(FScoreMeansAndVars, fp)", 
      "    fp.close()", 
      "", 
      "def printConfidences(FScoreList):", 
      "    \"\"\"", 
      "    Receives a list of (Fscore mean, Fscore stddev) tuples which it then processes sequentially", 
      "    and prints the confidences according to which the better measurements are significantly better", 
      "    than their counterparts.", 
      "    ", 
      "    @param FScoreList: a list of (FScoreMean, FScoreStdDev) tuples.", 
      "    @return: None", 
      "    @raise LogicalError: If FScoreList is None or empty. ", 
      "    \"\"\"", 
      "    ", 
      "    if FScoreList is None or len(FScoreList) == 0:", 
      "        raise LogicalError, \"Method %s: Cannot print the confidences over a null or empty list.\" %(inspect.stack()[0][3])", 
      "    ", 
      "    # We need to compare the bootstrapped F1 score of model 1 against any other model's,", 
      "    # then model 2's with every other model except 1, then 3's with 4, 5, 6, 7 and so on.", 
      "    ", 
      "    for i1 in range(len(FScoreList) - 1):", 
      "        for i2 in range(i1 + 1, len(FScoreList)):", 
      "            ", 
      "            # Get the mean F1 Score and the standard deviation of F-scores ", 
      "            # for both prediction models.", 
      "            ", 
      "            (mean1, stddev1) = FScoreList[i1]", 
      "            (mean2, stddev2) = FScoreList[i2]", 
      "            ", 
      "            # Find the model that does better.", 
      "            ", 
      "            maxMean, minMean = max(mean1, mean2), min(mean1, mean2)", 
      "            stddev = stddev1 if maxMean == mean1 else stddev2", 
      "            betterPredictor = i1 + 1 if maxMean == mean1 else i2 + 1", 
      "            ", 
      "            # Compute the confidence that the better performance of the", 
      "            # model was not by chance and print it.", 
      "            ", 
      "            confidence= computeConfidenceInterval(maxMean, stddev, minMean)", 
      "            print \"We compared models %d and %d.\" %(i1 + 1, i2 + 1)", 
      "            print \"There is a %1.f%% chance that the superior performance of model %d was not by chance.\" %(100*confidence, betterPredictor)", 
      "            ", 
      "        #end inner for loop", 
      "    # end outer for loop", 
      "</pre></code>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "We used the code above to compute the bootstrapped F-scores of *all* models over 1000 folds of the testing data. As usual, we have stored these bootstrapped", 
      "F-scores on disk. To perform our tests, we used the provided decision tree code. Of interest is the fact that we used two different tree structures ", 
      "for these tests: one with a *maximum* depth of 10 and one with a maximum depth of 20. Therefore, in the following code cell, we load two different bootstrap", 
      "F-scores from disk to memory:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "from code.util.bootstrapEval import *", 
      "import pickle as pkl", 
      "fp1 = open('proc_data/bootstrappedFScores_depth10.pda')# Bootstrapped F-score computed with a tree of maximum depth 10.", 
      "fp2 = open('proc_data/bootstrappedFScores_depth20.pda')# Same, but with a tree of maximum depth 20.", 
      "allDepth10FScores = pkl.load(fp1)", 
      "allDepth20FScores = pkl.load(fp2)", 
      "fp2.close()", 
      "fp1.close()"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 13
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Now that we have loaded those bootstrapped F-scores, of interest are two things (recall that our tests have occurred over 1000 folds of the testing data):", 
      "", 
      "1) For both tree configurations, which algorithm provided for the best mean bootstrapped F-score, and what was this F-score?", 
      "2) When comparing algorithm X and algorithm Y, either X or Y performed better: what is the probability that the better performance is statistically significant?", 
      "", 
      "Question (1) is easily respondible to, since we have the data in memory (of note is the fact that I couldn't understand how to use numpy.argmax on the bootstrapped", 
      "F-score lists, because they are lists of tuples, so I devised my own, inefficient and barely readable, argmax implementation). To respond to question (2), we assume", 
      "that the F-scores' distribution is approximately Gaussian and we followed the recipe of CIML, chapter 4, page 64."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "maximumFscore_depth10 = max(x[0] for x in allDepth10FScores)", 
      "maximumFscore_depth20 = max(x[0] for x in allDepth20FScores)", 
      "bestModel_depth10 = [x + 1 for x in range(len(allDepth10FScores)) if allDepth10FScores[x][0] == maximumFscore_depth10]  # couldn't find a ready-baked argmax() for this list case", 
      "bestModel_depth20 = [x + 1 for x in range(len(allDepth20FScores)) if allDepth20FScores[x][0] == maximumFscore_depth20]  # couldn't find a ready-baked argmax() for this list case", 
      "print \"For depth 10, The highest bootstrapped F-score over all was %.4f, observed by model %d.\" %(maximumFscore_depth10, bestModel_depth10[0])", 
      "printConfidences(allDepth10FScores)", 
      "print '\\n################################\\n'", 
      "print \"For depth 20, The highest bootstrapped F-score over all was %.4f, observed by model %d.\" %(maximumFscore_depth20, bestModel_depth20[0])", 
      "printConfidences(allDepth20FScores)", 
      "print '\\n################################\\n'"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 4
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "As is evident by the output, the increase of the maximum tree depth from 10 to 20 resulted in a different victor. For a tree of maximum depth 10, model VI", 
      "displayed the best mean F-score over 1000 folds, equal to about 67.05%. When we increased the maximum depth, model I came to be the victor, with a mean F-score", 
      "of 67.47%. The reader is encouraged to examine whichever model's mean F-score he would like, by indexing the \"allDepthxxFScores\" lists appropriately."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "###Ensemble Learning", 
      "", 
      "Now you will implement three ensemble learning methods to see if you can improve on results from the previous section. You should stick", 
      "to version VII of the recommender system (content-based + item-item + user-user predicted ratings) from this point forward. Continue using your", 
      "bootstraped F-score to perform comparisons below. Provide a sketch discussion (with code snippets if needed) below indicating how you", 
      "implemented each method."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Question 6: Implement Bagging where you bag decision trees. You should try to reuse your boottrsap code from the previous section. Compare bagged", 
      "trees to your results from Question 5. Comment on how the number of trees in the ensemble affects prediction performance. ", 
      "", 
      "###Response: ", 
      "", 
      "Note: For this question, we will use a decision tree depth of 20 because, as is shown from the following code snippet, model VII, on which we solely focus, performs slightly better", 
      "on the test data when trained on a tree with a maximum depth of 20."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "(q5mean10, q5stddev10) = allDepth10FScores[6]", 
      "(q5mean20, q5stddev20) = allDepth20FScores[6]", 
      "print q5mean10, q5mean20        "
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 8
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Given our \"dual\" implementation of the \"bootstrapEval\" method, we were able to use that method for the purposes of this question as well. ", 
      "Because training K-many decision trees is computationally expensive, we have once again pickled the trees to disk for future access. The following code snippet can be", 
      "used to compare the performance of the bagged decision trees against that of the - single decision tree - model VII. ", 
      "", 
      "**Please note:** execution of the", 
      "next code cell will take significant time and is likely to slow down your system, because the \"baggedClassifiers\" object which we have stored on ", 
      "disk is a 2.4 GB object, since it comprises a list of 50 decision trees of maximum depth 20. For this reason, we have \"toggled\" the output of the code cell", 
      "so that it shows the output that one would get *after* executing it. The reader is invited to verify the output by running the code cell itself,", 
      "but be warned that execution took 10 minutes on a custom system, with a quad-core Intel i3 processor, 6GB of ram and the following data of interest ", 
      "with respect to disk block readup speed:"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "<code><pre>", 
      "jason@debian:~/Dropbox/Course_Material/CMSC_726/PP03$ for i in 1 2 3; do sudo hdparm -tT /dev/sda; done", 
      "", 
      "/dev/sda:", 
      " Timing cached reads:   8928 MB in  2.00 seconds = 4467.22 MB/sec", 
      " Timing buffered disk reads: 262 MB in  3.01 seconds =  87.08 MB/sec", 
      "", 
      "/dev/sda:", 
      " Timing cached reads:   9098 MB in  2.00 seconds = 4551.59 MB/sec", 
      " Timing buffered disk reads: 276 MB in  3.01 seconds =  91.61 MB/sec", 
      "", 
      "/dev/sda:", 
      " Timing cached reads:   9112 MB in  2.00 seconds = 4558.91 MB/sec", 
      " Timing buffered disk reads: 272 MB in  3.01 seconds =  90.37 MB/sec", 
      "</pre></code>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "which yields an averaged cached read rate of 4525.9 MB/sec and an average buffered disk read rate of 89.68 MB/sec. Therefore, expect your computer to be ", 
      "bogged down for about 10 minutes, depending on the underlying technology."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "trainDat = pd.load('proc_data/prediction_ready_trainDat.pda')", 
      "testDat = pd.load('proc_data/prediction_ready_testDat.pda')", 
      "    ", 
      "fp = open('proc_data/baggedClassifiers.pda', 'rb')", 
      "classifierList = pkl.load(fp)", 
      "fp.close()", 
      "print \"We bagged %d classifiers.\" %(len(classifierList))", 
      "# For every classifier, get his prediction vector.", 
      "        ", 
      "predictionMatrix = np.array([tree.predict(testDat) for tree in classifierList]).transpose()", 
      "# Get the voted labels", 
      "votedLabels = [1 if predictionMatrix[j].tolist().count(1) > predictionMatrix[j].tolist().count(-1) else -1 for j in range(predictionMatrix.shape[0])]", 
      "        ", 
      "# Compute bagging F-score and compare it to sub-question VII's bootstrapped  F-score", 
      "        ", 
      "(bagmean, bagstddev) = bootStrapEval(testDat['isgood'].values, votedLabels, 1000)", 
      "print \"The mean F-score of our bagged classifiers over %d folds of the testing data was %.3f, and the standard deviation was %.3f.\" %(1000, bagmean, bagstddev)", 
      "fp = open('proc_data/bootstrappedFScores_depth20.pda')", 
      "bootstrappedFScores = pkl.load(fp)", 
      "fp.close()", 
      "(q5mean20, q5stddev20) = bootstrappedFScores[6]             # retrieve model VII's results", 
      "print \"In comparison, the bootstrapped F-score for sub-question VII of question 5 was %.3f and its standard deviation was %.3f.\" %(q5mean20, q5stddev20)", 
      "printConfidences([(bagmean, bagstddev), (q5mean20, q5stddev20)])"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 9
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "(In the output above, model \"1\" corresponds to the bagging model, and model \"2\" corresponds to model VII.)", 
      "", 
      "As is clearly evident, the bagged decision tree model leads to a huge improvement over the single decision tree of model VII. The bagging mean F-score is", 
      "8.4% higher than model VII's mean F-score."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Question 7: Implement Random Forest learning. This requires that you can construct \"random\" trees where splits are done over randomly chosen", 
      "subset of candidate splits. Feel free to extend the decision tree code we provide (look at `code.dectree.scoring.py`) to do this. Otherwise,", 
      "modify your PA01 decision tree code. You should be able to reuse your bagging code from Question 6 here. Compare random forests to the Bagging (Question 6)", 
      "and single Decision Tree (Question 5)."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Response:", 
      "", 
      "To answer the question, we performed slight changes to the provided decision tree code. These changes, namely, were:", 
      "", 
      "(1) We defined a small function called randomSubset() to generate a random subset of feature indices from an array of feature indices. The following markdown cell ", 
      "shows the implementation:"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "<code><pre>", 
      "def randomSubset(array):", 
      "    length = np.random.randint(1, len(array))", 
      "    return np.random.permutation(array)[:length]", 
      "</pre></code>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "The implementation is simple: First, we need to decide on the size of the subset. This is done by the first line, `length = np.random.randint(1, len(array))`.", 
      "Then, we need to decide on `length` - many random features to include in our new subset. This is accomplished by randomly permuting the entire feature array", 
      "and selecting the `length` first indices. ", 
      "", 
      "(2) We found the best split from within this randomly generated subset.", 
      "", 
      "The following markdown cell contains the relevant code snippet from the `DTree.train_helper()` method:"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "<code><pre>", 
      "temp = features_to_use.copy()", 
      "features_to_use = randomSubset(features_to_use)", 
      "split, _dummy, best_score = self.split_func(current_indexes, features_to_use)", 
      "features_left = temp", 
      "</pre></code>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "First, we make a temporary copy of our current features. Then, we select a random subset of these features, which we feed to \"split_func\" in order for it", 
      "to provide us with the best split from *within* the randomly generated subset. Finally, we retrieve the original feature vector and store it in the \"features_left\"", 
      "variable. ", 
      "", 
      "(3) We use the `features_left` variable, which contains the full feature vector after our operations desribed above, to make recursive calls of `train_helper` to the children nodes.", 
      "", 
      "The following MarkDown cell contains the relevant code snippet:"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "<code><pre>", 
      "left,left_depth = self.train_helper(features_left, current_indexes[split_indxs], depth+1, maxdepth,verbose=verbose)", 
      "right,right_depth = self.train_helper(features_left, current_indexes[split_indxs != True], depth+1, maxdepth,verbose=verbose)", 
      "</pre></code>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "In our recursive calls to the left and right children, we want to provide the full feature vector once again, so that the children", 
      "will be able to decide on a new randomly generated sub-vector of the *entire* feature vector, and so on.", 
      "", 
      "To respond to this question, we trained a random forest of 50 decision trees. We pickled the object to disk, which yielded a file slightly larger than the one ", 
      "of question 6. Therefore, the same computational considerations when running the following code cell apply and for this reason we have once again \"toggled\"", 
      "the output so that it is visible by default:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "trainDat = pd.load('proc_data/prediction_ready_trainDat.pda')", 
      "testDat = pd.load('proc_data/prediction_ready_testDat.pda')", 
      "fp = open('proc_data/baggedRandomClassifiers.pda', 'rb')", 
      "randomForest = pkl.load(fp)", 
      "fp.close()", 
      "print \"Our random forest contains %d trees.\" %(len(randomForest))", 
      "", 
      "# For every classifier, get his prediction vector.", 
      "        ", 
      "predictionMatrix = np.array([tree.predict(testDat) for tree in randomForest]).transpose()", 
      "# Get the voted labels", 
      "votedLabels = [1 if predictionMatrix[j].tolist().count(1) > predictionMatrix[j].tolist().count(-1) else -1 for j in range(predictionMatrix.shape[0])]", 
      "        ", 
      "# Compute bagging F-score and compare it to sub-question VII's bootstrapped  F-score", 
      "        ", 
      "(rfmean, rfstddev) = bootStrapEval(testDat['isgood'].values, votedLabels, 1000)", 
      "print \"The mean F-score of our random forest over %d folds of the testing data was %.3f, and the standard deviation was %.3f.\" %(1000, rfmean, rfstddev)", 
      "print \"In comparison, the mean F-score of the bagged decision trees over the same number of folds was %.3f, and the standard deviation was %.3f.\" %(bagmean, bagstddev)", 
      "print \"As a further reminder, the mean F-score of the single tree of question VII was %.3f and the standard deviation was %.3f.\" %(q5mean20, q5stddev20)", 
      "printConfidences([(rfmean, rfstddev), (bagmean, bagstddev), (q5mean20, q5stddev20)])"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 10
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "(In the output above, model 1 is our random forest model, model 2 is the bagged decision tree model, and model 3 is the single decision tree of question VII.)", 
      "", 
      "The conclusion that we draw is very promising. We observe that the mean F-score of the random forest classifier is within one mean of the bagged decision trees", 
      "model, for the same amount of trees and the same amount of testing data folds. If we combine this with the fact that a random forest trains much faster than a full-blown", 
      "bagged decision tree model (*), we see that we can approach the accuracy of the bagged model quite well under harsher time requirements.", 
      "", 
      "(*) We let the bagged decision tree model an entire night (9 hours) to train and store the classifiers on disk, whereas the random forest model needed about ", 
      "three hours or so. We assume that in the extreme case where at any possible splitting decision exactly one feature is considered at random (without any splitting ", 
      "criterion evaluated), the training time would be further reduced, not without paying a cost in F-measure. "
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Question 8: Implement AdaBoost for decision tree stumps (trees with a single split). You need to be able to train weighted decision tree stumps to ", 
      "do this. Feel free to extend the decision tree code we provide, or use your PA01 decision tree code. You should be able to define weighted", 
      "versions of the Gini index (if you use our code), or information gain (if you use your code) by changing how the proportion of examples from each class", 
      "are calculated at a given split. Compare boosted stumps to all previous methods."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Response:", 
      "", 
      "####An important note first: ", 
      "", 
      "", 
      "While our implementation of AdaBoost is 100% our own, the underlying idea of the grouping of examples not by their counts in a group but by the ", 
      "summation of their weights was yielded after a very lengthy discussion session with classmate Sarthak Grover, who was also struggling with AdaBoost at ", 
      "the time of the discussion. ", 
      "", 
      "We were able to implement AdaBoost in the nick of available time. Our implementation is located in the Python module `code.util.adaboost.py`. We are also", 
      "showing the relevant code in the following Markdown cell:"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "<code><pre>", 
      "from __future__ import division", 
      "'''", 
      "Created on Nov 17, 2012", 
      "", 
      "@author: jason", 
      "'''", 
      "", 
      "import pandas as pd", 
      "import numpy as np", 
      "import inspect", 
      "from dectree.DTree import get_tree", 
      "from mlExceptions import *", 
      "from util.bootstrapEval import bootStrapEval", 
      "", 
      "def adaboost(trainDat, K):", 
      "    ", 
      "    \"\"\"", 
      "    Implement the ADABoost algorithm, as described in CIML, page 152.", 
      "    @param trainDat: a pandas.DataFrame representing our training data.", 
      "    @param K: the number of decision tree stumps that we would like to train.", 
      "    @return --- a list of K decision tree stumps, trained on weighted data.", 
      "            --- a list of K adaptive parameters, used on predictions alongside", 
      "                the individual classifiers' predictions.", 
      "    @raise LogicalError if K<= 0, None or not an int", 
      "    @raise DatasetError if trainDat is None or empty", 
      "    \"\"\"", 
      "    ", 
      "    if trainDat is None or len(trainDat) == 0:", 
      "        raise DatasetError, \"Method %s: Cannot train ADAboost on a null or empty dataset.\" %(inspect.stack()[0][3])", 
      "    if K is None or K <= 0 or not isinstance(K, int):", 
      "        raise LogicalError, \"Method %s: Need to train a positive number of classifiers\" %(inspect.stack()[0][3])", 
      "    ", 
      "    print \"Starting AdaBoost algorithm.\"", 
      "    # initialize uniform weights", 
      "    ", 
      "    exampleWeights = np.array([(1 / trainDat.shape[0]) for _x_ in range(trainDat.shape[0])])", 
      "        ", 
      "    # run main algorithm ", 
      "    classifierList = list()", 
      "    adaptParams = list()", 
      "    for k in range(K):", 
      "        ", 
      "        # train a decision tree stump on the weighted training data", 
      "        print \"Training stump #%d.\" %(k+1)", 
      "        stump = get_tree(trainDat, 'isgood', exampleWeights, 1, 0)  ", 
      "        ", 
      "        # Run predictions on weighted training data ", 
      "        print \"Getting training data predictions for stump #%d.\" %(k+1)", 
      "        predictions = stump.predict(trainDat)", 
      "        ", 
      "        # Compute training error", 
      "        ", 
      "        trueValues = trainDat['isgood'].values", 
      "        ", 
      "        if len(predictions) != len(trueValues):", 
      "            raise LogicalError, \"Method %s, model #%d: predictions have to be as many as the true labels.\" %(inspect.stack()[0][3], k + 1)", 
      "        ", 
      "        ", 
      "        misclassifiedExampleWeights = [exampleWeights[n] for n in range(len(predictions)) if predictions[n] != trueValues[n]]", 
      "        trainingError = np.sum(misclassifiedExampleWeights)   # this is how we consider weighted training error in AdaBoost.", 
      "        ", 
      "        # Compute and store the \"adaptive\" parameter a(k)", 
      "        ", 
      "        currentAdaptParam = 0.5 * np.log((1 - trainingError) / trainingError)", 
      "        ", 
      "        #if type(currentAdaptParam) != float:", 
      "            #raise LogicalError, \"Method %s, model #%d: type of adaptive parameter was %s instead of float.\" %(inspect.stack()[0][3], k + 1, type(currentAdaptParam))", 
      "        ", 
      "        adaptParams.append(currentAdaptParam)", 
      "        print \"Computed adaptive parameter for classifier %d. It is equal to: %.4f\" %(k+1, currentAdaptParam)", 
      "         ", 
      "        # Update and normalize example weights", 
      "        # Note that this is not a dot product, but an element-wise multiplication.", 
      "        ", 
      "        exponent = -currentAdaptParam *np.array([trueValues[n] for n in range(trainDat.shape[0])])* np.array([predictions[n] for n in range(trainDat.shape[0])])", 
      "        ", 
      "        try:", 
      "            len(exponent)", 
      "        except TypeError:", 
      "            raise LogicalError, \"Method %s: \\\"exponent\\\" is not an iterable.\" %(inspect.stack()[0][3]) ", 
      "        if len(exponent) != trainDat.shape[0]:", 
      "            raise LogicalError, \"Method %s: our derivation of \\\"exponent\\\" should've yielded a numpy.ndarray of size %d at this point.\" %(inspect.stack()[0][3], trainDat.shape[0])", 
      "        ", 
      "        multiplier = exampleWeights * np.exp(exponent)", 
      "        ", 
      "        try:", 
      "            len(multiplier)", 
      "        except TypeError:", 
      "            raise LogicalError, \"Method %s: \\\"multiplier\\\" is not an iterable.\" %(inspect.stack()[0][3]) ", 
      "        ", 
      "        if len(multiplier) != trainDat.shape[0]:", 
      "            raise LogicalError, \"Method %s: our derivation of \\\"multiplier\\\" should've yielded a numpy.ndarray of size %d at this point.\" %(inspect.stack()[0][3], trainDat.shape[0])", 
      "        ", 
      "        # Now we need to normalize, and God only knows how we're supposed to do this.", 
      "        ", 
      "        normalizer = np.sum(multiplier)             # TODO: Decide whether this is the correct normalizer    ", 
      "        exampleWeights = exampleWeights / normalizer   ", 
      "        ", 
      "        try:", 
      "            len(exampleWeights)", 
      "        except TypeError:", 
      "            raise LogicalError, \"Method %s, model #%d: after the update to \\\"exampleWeights\\\", this variable no longer represents a numpy.ndarray.\" %(inspect.stack()[0][3], k + 1)", 
      "        if  len(exampleWeights) != trainDat.shape[0]:", 
      "            raise LogicalError, \"Method %s, model #%d: the update to exampleWeights should've yielded a numpy.ndarray of size %d at this point.\" %(inspect.stack()[0][3], k + 1, trainDat.shape[0])", 
      "        ", 
      "    return classifierList, adaptParams", 
      "</pre></code>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "In the above code, notice that we are making a call to the function `get_tree` by supplying the weight vector that AdaBoost maintains. This suggests that we", 
      "must have somehow tweaked the provided decision tree code and this is indeed what we have done. In essence, the presence of one weight per example means that ", 
      "the distribution from which our training data has originated has radically changed. So, whenever we compute class proportions in our code in order to find the", 
      "best split through the gini index, we need to compute sums of example weights instead of counts of examples themselves. Roughly, we have achieved to do this", 
      "by replacing every call to pandas.DataFrame.groupby via our custom \"weightedGroupBy()\" implementation, shown below and made available in Python module `code.util.scoring.py`:"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "<code><pre>", 
      "\"\"\"", 
      "Utilities for scoring splits in a decision tree", 
      "\"\"\"", 
      "", 
      "import numpy as np", 
      "import pandas as pd", 
      "import inspect", 
      "from util.mlExceptions import DatasetError, LogicalError", 
      "from Node import Split", 
      "", 
      "def weightedGroupBy(df, columns, weights):", 
      "    ", 
      "    '''", 
      "    returns a weighted version of groupby(columns).size(),", 
      "            i.e the otherwise crisp \"size()\" of every group is now a sum of the weights", 
      "            of the examples of that group (instead of a mere count of the examples of that group).", 
      "    @param df: a pandas.DataFrame which holds our data", 
      "    @param columns: a list of strings which define the columns that we will use as the keys", 
      "            for the pandas.Series object that we will return. In groupby terms, it is those ", 
      "            columns that we \"group by\".", 
      "    @param weights: a numpy.ndarray which holds the weights for each training example.", 
      "    @return a pandas.Series with a MultiIndex of \"columns\" and mapped", 
      "            values corresponding to the sums of weights of the examples grouped by the MultiIndex.", 
      "            Think of it as a weighted version of the pandas.Series produced by the call \"df.groupby(columns).size()\".", 
      "    @raise DatasetError if provided with a null or empty dataset, LogicalError in various other cases.", 
      "    @attention Please note that the implementation of this method was only made feasible after a lengthy conversation", 
      "            with fellow classmate Sarthak Grover, who pointed out that the current way that class proportions are", 
      "            computed (based on pandas.DataFrame.groupby()) does not really account for weighted data, and we should ", 
      "            thus try to \"inject\" this characteristic in the decision tree and scoring code.", 
      "    '''", 
      "    ", 
      "    if df is None or df.shape[0] == 0:", 
      "        raise DatasetError, \"Method %s: Please provide a non-empty dataset.\" %(inspect.stack()[0][3])", 
      "    ", 
      "    wsumhash = dict()", 
      "    groups = df.groupby(columns).groups", 
      "    ", 
      "    ", 
      "    for key in groups.iterkeys():           # key can be a tuple, it can be a String, an int, anything", 
      "        ", 
      "        # Index self.weights by the currently mapped example indices", 
      "        # to retrieve only the weights of the examples that interest us", 
      "        ", 
      "        currentWeights = weights[groups[key]]      # \"Fancy\" indexing of a numpy.ndarray", 
      "        ", 
      "        # Insert a new entry to the hash table, with the same key", 
      "        # but with a mapped value corresponding to a sum of the ", 
      "        # respective example weights.", 
      "        ", 
      "        wsumhash[key] = np.sum(currentWeights)", 
      "    ", 
      "        ", 
      "    # Make a pandas.Series out of the dictionary and return it.", 
      "    ", 
      "    keys = [key for key in groups.iterkeys()]", 
      "    if len(keys) == 0: ", 
      "        raise LogicalError, \"Method %s: There should be at least one key in the dictionary.\" %(inspect.stack()[0][3])", 
      "    ", 
      "    # VERY IMPORTANT: Make the series indexed by a MultiIndex ONLY IF keys", 
      "    # is a list of tuples. Otherwise, let it have its own int index", 
      "    if isinstance(keys[0], tuple): ", 
      "        multiIndex = pd.MultiIndex.from_tuples(keys)", 
      "        retVal = pd.Series(wsumhash, index = multiIndex)", 
      "    else:", 
      "        # if not isinstance(keys[0], int):", 
      "            #raise LogicalError, \"Method %s: Keys can only be tuples or ints.\" %(inspect.stack()[0][3])", 
      "        retVal = pd.Series(wsumhash)", 
      "    return retVal  ", 
      "</pre></code>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "We have thus proceeded to enhance the provided decision tree with the weight vector which is maintained and constantly updated by AdaBoost, and we substituted", 
      "every call to pandas.DataFrame.groupby() by a call to the method above. Notice that this method requires that the \"grouping by\" is done over a list of ", 
      "arbitrary length: if the length is 1, then the returned pandas.Series will be indexed by a simple integer index: if, however, we are \"grouping by\" a list ", 
      "of more than 1 columns, then the returned pandas.Series will be indexed by a multi-level pandas.MultiIndex, which we build on the spot from the ", 
      "dictionary keys.", 
      "", 
      "We have once again proceeded to store all classifiers on disk, along with their adaptive parameters. The following code cell will give us an answer as to ", 
      "how AdaBoost performs in this dataset, once again over K = 1000 folds of the testing data. Because the pickled object is very small, this code cell executes", 
      "quite fast, so we have not \"toggled\" its output:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "trainDat = pd.load('proc_data/prediction_ready_trainDat.pda')", 
      "testDat = pd.load('proc_data/prediction_ready_testDat.pda')", 
      "fp1 = open('proc_data/boostedClassifiers.pda', 'rb')", 
      "fp2 = open('proc_data/adaptiveParams.pda', 'rb')", 
      "boostedTrees = pkl.load(fp1)", 
      "adaptiveParams = pkl.load(fp2)", 
      "adaboostPredictionMatrix = np.array([tree.predict(testDat) for tree in boostedTrees]).transpose()", 
      "adaboostVotedLabels = [np.sign(np.sum(adaptiveParams * adaboostPredictionMatrix[j])) for j in range(adaboostPredictionMatrix.shape[0])]", 
      "", 
      "# Compute mean and stddev of F-score over 1000 folds:", 
      "print \"Computing mean and std. dev of F-score over 1000 folds...\"", 
      "(boostmean, booststddev) = bootStrapEval(testDat['isgood'].values, adaboostVotedLabels, 1000)", 
      "print \"Mean F-score of AdaBoost: %.4f. Mean standard deviation: %.4f\" %(boostmean, booststddev)", 
      "print \"In comparison, the mean F-scores of random forests, bagging and model VII were, respectively:\",", 
      "print \"%.4f, %.4f, %.4f\" %(rfmean, bagmean, q5mean20)", 
      "printConfidences([(boostmean, booststddev), (rfmean, rfstddev), (bagmean, bagstddev), (q5mean20, q5stddev20)])"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 14
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "As we can see, the results of AdaBoost are very promising! With only 50 decision *stumps* trained, we have been able to outperform all the other ensemble learning ", 
      "methods. The fact that those 50 decision stumps took less than 15 minutes to train (given that they are exactly that: stumps) in our machine is also ", 
      "a very promising direction for future tests with this algorithm, because it indicates that we can train a very large number of stumps in a very short amount", 
      "of time, thus increasing our test-time performance."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "###Handing in", 
      "", 
      "As before, you should run all heavy computations outside of the IPython notebook. However, all code you run to include in your discussion (e.g., to", 
      "make plots, compute bootstrapped f-score from saved predictions) can be run inside the notebook. Please submit your source code along with", 
      "this notebook."
     ]
    }
   ]
  }
 ]
}